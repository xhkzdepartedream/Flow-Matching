# Flow-Matching: Efficient Continuous-Time Diffusion Models

[English](README.md) | [ä¸­æ–‡](README_zh.md)

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.17110046.svg)](https://doi.org/10.5281/zenodo.17110046)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-red.svg)](https://pytorch.org/)
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)

A lightweight and efficient PyTorch framework for **Flow-Matching**, a modern approach to continuous-time diffusion models. This project demonstrates how to achieve high-quality image generation with significantly fewer parameters and computational resources.

<p align="center">
  <img src="visuals_celebahq/myplot12.png" width="200"/>
  <img src="visuals_celebahq/plot_2025-08-07 13-30-26_7.png" width="200"/> 
</p>

Download our trained model:
[Click to Download Model Weights](https://drive.google.com/file/d/1jzkRGL_ZqgXaFskdXpLxoU__KSgnMz4X/view?usp=drive_link)

## ğŸ† Performance Highlights

This project demonstrates exceptional efficiency, achieving a strong FID score on a high-resolution dataset with a remarkably small model and short training time.

-   **Dataset**: CelebA-HQ 256x256
-   **FID Score**: **47.39**
-   **Model**: DiT (Diffusion Transformer) with only **38.75M** parameters
-   **Training Time**: Just **~20 hours** on 6x NVIDIA RTX 4090 GPUs

These results highlight the power of combining Flow-Matching with an efficient DiT architecture in the latent space, making high-quality generative modeling more accessible.

## ğŸš€ Key Features

-   **Efficient & Fast**: Achieves an FID of 47.39 on CelebA-HQ 256x256 with only ~20 hours of training.
-   **Lightweight Model**: The DiT model has only **38.75M** parameters, significantly smaller than typical models for this task.
-   **Cutting-Edge Architecture**: Implements a state-of-the-art framework combining Flow-Matching with a Diffusion Transformer (DiT) in the VAE latent space.
-   **Modular and Config-Driven**: Easily experiment with different models and schedulers using YAML configuration files.
-   **Multi-GPU Support**: Includes optimized scripts for distributed training and sampling.

## ğŸ”§ Model Configuration & Training

This project's success lies in its efficient model configuration. We use a two-stage approach:

1.  **VAE Compression**: A fine-tuned `AutoencoderKL` (VAE) is used to compress high-resolution 256x256x3 images from the CelebA-HQ dataset into a much smaller 32x32x4 latent representation. This significantly reduces the computational load for the diffusion model.

2.  **Diffusion Transformer (DiT)**: A `DiT` model with the following configuration is trained on the latent space:
    -   **Input Size**: 32x32
    -   **Input Channels**: 4
    -   **Patch Size**: 2
    -   **Model Dimension**: 512
    -   **Transformer Blocks**: 8
    -   **Attention Heads**: 16
    -   **Total Parameters**: **38.75M**

This setup allows us to achieve high-quality image generation while minimizing computational and memory requirements, making it an excellent example of efficient diffusion model design.

## ğŸ“ Project Structure

This project adopts a highly modular architecture to ensure a clear separation of concerns, enhancing scalability and maintainability. This design makes it easy to experiment with new models, data, or training procedures.

```
Flow-Matching/
â”œâ”€â”€ configs/              # Experiment configurations (YAML files)
â”‚   â””â”€â”€ celebahq_dit.yaml
|
â”œâ”€â”€ data_processing/      # Scripts for data downloading and preprocessing
â”‚   â”œâ”€â”€ download_celeba_hq.py
â”‚   â””â”€â”€ init_dataset.py
|
â”œâ”€â”€ fm_scheduler/         # Core Flow Matching logic
â”‚   â”œâ”€â”€ FlowsBase.py      # Base classes for flow definitions
â”‚   â”œâ”€â”€ OTScheduler.py    # Optimal Transport schedulers
â”‚   â””â”€â”€ Sampler.py        # ODE/SDE samplers
|
â”œâ”€â”€ models/               # Core neural network architectures
â”‚   â”œâ”€â”€ DiT.py            # Diffusion Transformer (DiT) model
â”‚   â””â”€â”€ Unet.py           # UNet model
|
â”œâ”€â”€ modules/              # Reusable components and sub-modules
â”‚   â”œâ”€â”€ autoencoderkl.py  # VAE model for latent space encoding/decoding
â”‚   â”œâ”€â”€ embedders.py      # Text and class embedders
â”‚   â””â”€â”€ perceptual_module.py # Perceptual loss modules (e.g., VGG)
|
â”œâ”€â”€ scripts/              # Executable scripts for various tasks
â”‚   â”œâ”€â”€ train/            # Training scripts
â”‚   â”‚   â””â”€â”€ train.py
â”‚   â”œâ”€â”€ compute_fid.py    # FID score computation
â”‚   â””â”€â”€ sample_celebahq.py # Image generation scripts
|
â””â”€â”€ trainer/              # High-level training orchestration
    â”œâ”€â”€ Trainer.py        # Main trainer class for the generative model
    â””â”€â”€ AutoencoderKL_trainer.py # Trainer for the VAE
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.8 or higher
- CUDA-compatible GPU (recommended)

### Quick Setup

1.  **Clone the repository**
```bash
git clone https://github.com/your-username/Flow-Matching.git
cd Flow-Matching
```
And download the trained model from the link above.

2.  **Install dependencies**
```bash
pip install -r requirements.txt
```

## ğŸš€ Quick Start

### 1. Choose Your Configuration

Select a configuration from the `configs/` directory. For example, `celebahq_dit.yaml` is configured for the CelebA-HQ dataset.

### 2. Generate Samples

Use the `scripts/batch_sample.py` script to generate images. You can run it in single-GPU or multi-GPU mode.

**Single-GPU:**
```bash
python scripts/batch_sample.py
```

**Multi-GPU (e.g., with 2 GPUs):**
```bash
torchrun --nproc_per_node=2 scripts/batch_sample.py
```

## ğŸ“š Citation

If you use this code in your research, please cite the original papers and this repository.

## ğŸ“„ License

This project is licensed under the MIT License.
