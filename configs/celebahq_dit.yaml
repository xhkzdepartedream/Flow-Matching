trainer:
  target: trainer.Trainer.Trainer
  params:
    title: "celebahq_dit"
    n_epoch: 1000
    batch_size: 128
    lr: 3.0e-4
    gradient_accumulation_steps: 4
    warmup_steps: 500
    ignore_labels: false
    model:
      target: models.DiT.DiT
      params:
        input_size: 32
        input_ch: 4
        patch_size: 2
        n_ch: 512
        n_blocks: 8
        num_heads: 16
        pe: "rope"
        has_cond: false
    scheduler:
      target: fm_scheduler.OTScheduler.OTScheduler
      params: {}
    dataset:
      target: data_processing.init_dataset.LatentDataset
      params:
        lmdb_path: "../data/celebahq_latents.lmdb/"

checkpoint:
  load: true
  path: "../celebahq_dit_1000.pth"
  save_every: 100


# --- Sampling Arguments ---
sampling:
  shape: [16, 4, 32, 32]
  n_steps: 200
  sampler_type: 'midpoint'
  output_dir: "./samples_cifar10"
  model_path: "../celebahq_dit_1000.pth"
  vae_path: "../autoencoderkl_finetuned_celeba_hq2/"


